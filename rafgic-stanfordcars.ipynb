{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":46697,"sourceType":"datasetVersion","datasetId":31559}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"'''\n1. Embed all the train images using CLIP embedding and store in the FAISS VectorDB\n2. Now train all the images while image encoded using VGG19 and cross product with corresponding top 1 CLIP embedding\n3. Forward to classifier layer for classification \n4. During test, for each test image gectc\n'''","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-01T17:12:38.891801Z","iopub.execute_input":"2024-05-01T17:12:38.892195Z","iopub.status.idle":"2024-05-01T17:12:38.899287Z","shell.execute_reply.started":"2024-05-01T17:12:38.892167Z","shell.execute_reply":"2024-05-01T17:12:38.898256Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"'\\n1. Embed all the train images using CLIP embedding and store in the FAISS VectorDB\\n2. Now train all the images while image encoded using VGG19 and cross product with corresponding top 1 CLIP embedding\\n3. Forward to classifier layer for classification \\n4. During test, for each test image gectc\\n'"},"metadata":{}}]},{"cell_type":"code","source":"!pip install efficientnet_pytorch","metadata":{"execution":{"iopub.status.busy":"2024-05-02T01:45:19.587820Z","iopub.execute_input":"2024-05-02T01:45:19.588160Z","iopub.status.idle":"2024-05-02T01:45:36.035067Z","shell.execute_reply.started":"2024-05-02T01:45:19.588131Z","shell.execute_reply":"2024-05-02T01:45:36.033904Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting efficientnet_pytorch\n  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from efficientnet_pytorch) (2.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->efficientnet_pytorch) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->efficientnet_pytorch) (1.3.0)\nBuilding wheels for collected packages: efficientnet_pytorch\n  Building wheel for efficientnet_pytorch (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for efficientnet_pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16428 sha256=f2f532b1ed81c9454c45c7ecd638a55ac4180b100193e6141be8ac1861712de9\n  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\nSuccessfully built efficientnet_pytorch\nInstalling collected packages: efficientnet_pytorch\nSuccessfully installed efficientnet_pytorch-0.7.1\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install faiss-cpu","metadata":{"execution":{"iopub.status.busy":"2024-05-02T05:12:08.690729Z","iopub.execute_input":"2024-05-02T05:12:08.691104Z","iopub.status.idle":"2024-05-02T05:12:25.305732Z","shell.execute_reply.started":"2024-05-02T05:12:08.691073Z","shell.execute_reply":"2024-05-02T05:12:25.304541Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting faiss-cpu\n  Downloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from faiss-cpu) (1.26.4)\nDownloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-cpu\nSuccessfully installed faiss-cpu-1.8.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<hr>\n<hr>\n<h2>Fine tuning of EffecientNet Model</h2>\n<hr>\n<hr>","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport faiss\nimport torch.nn as nn\nimport torch.optim as optim\nfrom transformers import CLIPProcessor, CLIPModel\nfrom torchvision import datasets, transforms\n#from efficientnet_pytorch import EfficientNet\n\nlen(os.listdir(\"/kaggle/input/stanford-car-dataset-by-classes-folder/car_data/car_data/train/\"))","metadata":{"execution":{"iopub.status.busy":"2024-05-02T05:12:31.151319Z","iopub.execute_input":"2024-05-02T05:12:31.151888Z","iopub.status.idle":"2024-05-02T05:12:38.430117Z","shell.execute_reply.started":"2024-05-02T05:12:31.151839Z","shell.execute_reply":"2024-05-02T05:12:38.428993Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"196"},"metadata":{}}]},{"cell_type":"code","source":"# Define transforms for the dataset\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Load your custom dataset using torchvision.datasets.ImageFolder\ntrain_dataset = datasets.ImageFolder(root='/kaggle/input/stanford-car-dataset-by-classes-folder/car_data/car_data/train/', transform=transform)\nval_dataset = datasets.ImageFolder(root='/kaggle/input/stanford-car-dataset-by-classes-folder/car_data/car_data/test/', transform=transform)\n\n# Define DataLoader\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load pre-trained EfficientNet model\nmodel = EfficientNet.from_pretrained('efficientnet-b3', num_classes=196)  # Change num_classes according to your dataset\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\nnum_epochs = 10\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nprint()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\n# Define early stopping parameters\npatience = 3  # Number of epochs to wait for improvement\nearly_stopping_counter = 0\nbest_val_loss = np.Inf  # Initialize with positive infinity\n\nfor epoch in range(num_epochs):\n    # Training loop\n    model.train()\n    running_loss = 0.0\n    for i, (inputs, labels) in enumerate(train_loader, 1):\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * inputs.size(0)\n        \n        if i % 10 == 0:  # Print every 10 mini-batches\n            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i}/{len(train_loader)}], Loss: {loss.item():.4f}')\n    \n    epoch_loss = running_loss / len(train_dataset)\n    \n    # Validation loop\n    model.eval()\n    val_loss = 0.0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item() * inputs.size(0)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    val_loss /= len(val_dataset)\n    val_accuracy = 100 * correct / total\n    \n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n    \n    # Check for improvement in validation loss\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        early_stopping_counter = 0\n        # Save the best model\n        torch.save(model.state_dict(), 'efficientnet_finetuned_best.pth')\n    else:\n        early_stopping_counter += 1\n        if early_stopping_counter >= patience:\n            print(f'Validation loss did not improve for {patience} epochs. Early stopping...')\n            break\n\n# Load the best model\nmodel.load_state_dict(torch.load('efficientnet_finetuned_best.pth'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<hr>\n<hr>\n<h1>RAFGIC</h1>\n<hr>\n<hr>","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\n\nttransform = transforms.ToTensor()\n\nclass CustomDataset(Dataset):\n    def __init__(self, dataset):\n        self.dataset = dataset\n\n    def __len__(self):\n        return len(self.dataset)\n    \n    def __getitem__(self, idx):\n        # Get original data from the dataset\n        original_data, label = self.dataset[idx]\n        \n        inputs = clip_processor(text=None, images=original_data, return_tensors=\"pt\")[\"pixel_values\"].to(device)\n        outputs = clip_model.get_image_features(inputs.cuda())\n        D, I = index_flat.search(outputs.cpu().detach().numpy(), 1)  # actual search\n        outputs = index_flat.reconstruct(int(I[0][0]))   \n        return ttransform(original_data),torch.tensor(outputs), label\n","metadata":{"execution":{"iopub.status.busy":"2024-05-02T05:12:38.431890Z","iopub.execute_input":"2024-05-02T05:12:38.432317Z","iopub.status.idle":"2024-05-02T05:12:38.440608Z","shell.execute_reply.started":"2024-05-02T05:12:38.432290Z","shell.execute_reply":"2024-05-02T05:12:38.439666Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport requests\nfrom io import BytesIO\n\n# URL of the raw numpy file on GitHub\nurl = 'https://github.com/Lordvarun23/RAFGIC/raw/main/train_clip_embeddings.npy'\n\n# Send a GET request to the URL\nresponse = requests.get(url)\n\n# Check if the request was successful (status code 200)\nif response.status_code == 200:\n    # Read the content of the response\n    content = BytesIO(response.content)\n    \n    # Load the numpy array from the content\n    embeddings = np.load(content)\n\n\nres = faiss.IndexFlatL2()  # use a single GPU\n\n## Using a flat index\n\nindex_flat = faiss.IndexFlatL2(768)  # build a flat (CPU) index\n\nindex_flat.add(embeddings)         # add vectors to the index\nprint(index_flat.ntotal)","metadata":{"execution":{"iopub.status.busy":"2024-05-02T05:12:38.441815Z","iopub.execute_input":"2024-05-02T05:12:38.442095Z","iopub.status.idle":"2024-05-02T05:12:39.475045Z","shell.execute_reply.started":"2024-05-02T05:12:38.442072Z","shell.execute_reply":"2024-05-02T05:12:39.473930Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"8144\n","output_type":"stream"}]},{"cell_type":"code","source":"'''k = 1                          # we want to see 4 nearest neighbors\nD, I = index_flat.search(np.array([embeddings[1]]), k)  # actual search\nI[0][0]'''","metadata":{"execution":{"iopub.status.busy":"2024-05-02T05:12:39.477086Z","iopub.execute_input":"2024-05-02T05:12:39.477417Z","iopub.status.idle":"2024-05-02T05:12:39.484701Z","shell.execute_reply.started":"2024-05-02T05:12:39.477391Z","shell.execute_reply":"2024-05-02T05:12:39.483711Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'k = 1                          # we want to see 4 nearest neighbors\\nD, I = index_flat.search(np.array([embeddings[1]]), k)  # actual search\\nI[0][0]'"},"metadata":{}}]},{"cell_type":"code","source":"#index_flat.reconstruct(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\nclip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")","metadata":{"execution":{"iopub.status.busy":"2024-05-02T05:12:41.744659Z","iopub.execute_input":"2024-05-02T05:12:41.745055Z","iopub.status.idle":"2024-05-02T05:13:05.197702Z","shell.execute_reply.started":"2024-05-02T05:12:41.745024Z","shell.execute_reply":"2024-05-02T05:13:05.196776Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.52k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"951fc39fb0ec4283b0336be53c89c2d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.71G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cc790c2847e49299bafad948ec119e0"}},"metadata":{}},{"name":"stderr","text":"2024-05-02 05:12:53.759055: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-02 05:12:53.759213: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-02 05:12:53.888750: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c765f623b09451b88f1587e1bc01cfa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/905 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bdbdcfc12344896a536d48fd7b75c90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/961k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ddcc80043294a5a97e47e2881fd5e16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"228acd59981045f8b2d3e47e35963606"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ac0c885b614411b8cb2660471f6cd3f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1983e9193ed49f0920e3e442a70cd59"}},"metadata":{}}]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nclip_model.to(device)\nprint()","metadata":{"execution":{"iopub.status.busy":"2024-05-02T05:13:05.199403Z","iopub.execute_input":"2024-05-02T05:13:05.199950Z","iopub.status.idle":"2024-05-02T05:13:05.889705Z","shell.execute_reply.started":"2024-05-02T05:13:05.199923Z","shell.execute_reply":"2024-05-02T05:13:05.888434Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"'''import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\n\nclass CustomModel(nn.Module):\n    def __init__(self, num_classes):\n        super(CustomModel, self).__init__()\n        # Load pre-trained VGG19 model\n        self.vgg19 = models.vgg19(pretrained=True)\n        \n        # Define the linear layer\n        self.linear1 = nn.Linear(512 * 7 * 7, 768)\n        \n        # Define the linear layer\n        self.linear2 = nn.Linear(768, 512)\n        \n        # Define the classification layer\n        self.classification_layer = nn.Linear(512, num_classes)\n    \n    def forward(self, x,embedding):\n        # Pass input through VGG19 model\n        image = x\n        x = self.vgg19.features(x)\n        x = self.vgg19.avgpool(x)\n        x = torch.flatten(x, 1)\n        \n        # Pass through linear layer\n        x = self.linear1(x)\n        \n        # Apply custom function\n        # Reshape tensors for batch-wise matrix multiplication\n        x = x * embedding\n        \n        x = self.linear2(x)\n        \n        # Forward to classification layer\n        x = self.classification_layer(x)\n        \n        return x\n\nrafgic_model = CustomModel(num_classes=196)  # Set num_classes according to your dataset\n# Train the model\nnum_epochs = 10\nrafgic_model.to(device)\nprint()'''","metadata":{"execution":{"iopub.status.busy":"2024-05-02T04:33:17.857373Z","iopub.execute_input":"2024-05-02T04:33:17.857766Z","iopub.status.idle":"2024-05-02T04:33:19.895562Z","shell.execute_reply.started":"2024-05-02T04:33:17.857734Z","shell.execute_reply":"2024-05-02T04:33:19.894592Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\n\nclass CustomModel(nn.Module):\n    def __init__(self, num_classes):\n        super(CustomModel, self).__init__()\n        # Load pre-trained VGG19 model\n        self.vgg19 = models.vgg19(pretrained=True)\n        \n        # Define the linear layer\n        self.linear1 = nn.Linear(512 * 7 * 7, 768)\n        \n        # Define BatchNorm layer\n        self.batchnorm1 = nn.BatchNorm1d(768)\n        \n        # Define Dropout layer\n        self.dropout1 = nn.Dropout(0.2)\n        \n        # Define the linear layer\n        self.linear2 = nn.Linear(768, 512)\n        \n        # Define the classification layer\n        self.classification_layer = nn.Linear(512, num_classes)\n    \n    def forward(self, x, embedding):\n        # Pass input through VGG19 model\n        image = x\n        x = self.vgg19.features(x)\n        x = self.vgg19.avgpool(x)\n        x = torch.flatten(x, 1)\n        \n        # Pass through linear layer\n        x = self.linear1(x)\n        x = self.batchnorm1(x)\n        x = F.relu(x)\n        x = self.dropout1(x)\n        \n        # Apply custom function\n        # Reshape tensors for batch-wise matrix multiplication\n        x = x * embedding\n        \n        x = self.linear2(x)\n        \n        # Forward to classification layer\n        x = self.classification_layer(x)\n        \n        return x\n\nrafgic_model = CustomModel(num_classes=196)  # Set num_classes according to your dataset\n# Train the model\nnum_epochs = 10\nrafgic_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-02T05:13:43.770557Z","iopub.execute_input":"2024-05-02T05:13:43.770967Z","iopub.status.idle":"2024-05-02T05:13:49.661209Z","shell.execute_reply.started":"2024-05-02T05:13:43.770935Z","shell.execute_reply":"2024-05-02T05:13:49.660255Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n100%|██████████| 548M/548M [00:03<00:00, 162MB/s]  \n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"CustomModel(\n  (vgg19): VGG(\n    (features): Sequential(\n      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): ReLU(inplace=True)\n      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (3): ReLU(inplace=True)\n      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (6): ReLU(inplace=True)\n      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (8): ReLU(inplace=True)\n      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (11): ReLU(inplace=True)\n      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (13): ReLU(inplace=True)\n      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (15): ReLU(inplace=True)\n      (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (17): ReLU(inplace=True)\n      (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (20): ReLU(inplace=True)\n      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (22): ReLU(inplace=True)\n      (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (24): ReLU(inplace=True)\n      (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (26): ReLU(inplace=True)\n      (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (29): ReLU(inplace=True)\n      (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (31): ReLU(inplace=True)\n      (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (33): ReLU(inplace=True)\n      (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (35): ReLU(inplace=True)\n      (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    )\n    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n    (classifier): Sequential(\n      (0): Linear(in_features=25088, out_features=4096, bias=True)\n      (1): ReLU(inplace=True)\n      (2): Dropout(p=0.5, inplace=False)\n      (3): Linear(in_features=4096, out_features=4096, bias=True)\n      (4): ReLU(inplace=True)\n      (5): Dropout(p=0.5, inplace=False)\n      (6): Linear(in_features=4096, out_features=1000, bias=True)\n    )\n  )\n  (linear1): Linear(in_features=25088, out_features=768, bias=True)\n  (batchnorm1): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (dropout1): Dropout(p=0.2, inplace=False)\n  (linear2): Linear(in_features=768, out_features=512, bias=True)\n  (classification_layer): Linear(in_features=512, out_features=196, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"# Define transforms for the dataset\ntransform = transforms.Compose([transforms.Resize((224, 224))])\n\n# Load your custom dataset using torchvision.datasets.ImageFolder\ntrain_dataset = datasets.ImageFolder(root='/kaggle/input/stanford-car-dataset-by-classes-folder/car_data/car_data/train/', transform=transform)\nval_dataset = datasets.ImageFolder(root='/kaggle/input/stanford-car-dataset-by-classes-folder/car_data/car_data/test/', transform=transform)\n\n\ncustom_train_dataset = CustomDataset(train_dataset)\ncustom_test_dataset = CustomDataset(val_dataset)\n\ntrain_loader = DataLoader(custom_train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(custom_test_dataset, batch_size=32, shuffle=False)\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(rafgic_model.parameters(), lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2024-05-02T05:13:56.739322Z","iopub.execute_input":"2024-05-02T05:13:56.740223Z","iopub.status.idle":"2024-05-02T05:14:05.413328Z","shell.execute_reply.started":"2024-05-02T05:13:56.740187Z","shell.execute_reply":"2024-05-02T05:14:05.412535Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"'''# Define DataLoader\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\n# Define early stopping parameters\npatience = 8  # Number of epochs to wait for improvement\nearly_stopping_counter = 0\nbest_val_loss = np.Inf  # Initialize with positive infinity\n\nfor epoch in range(num_epochs):\n    # Training loop\n    rafgic_model.train()\n    running_loss = 0.0\n    for i, (inputs,embeddings, labels) in enumerate(train_loader, 1):\n        inputs,embeddings, labels = inputs.to(device),embeddings.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = rafgic_model(inputs,embeddings)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * inputs.size(0)\n        \n        if i % 10 == 0:  # Print every 10 mini-batches\n            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i}/{len(train_loader)}], Loss: {loss.item():.4f}')\n    \n    epoch_loss = running_loss / len(train_dataset)\n    \n    # Validation loop\n    rafgic_model.eval()\n    val_loss = 0.0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs,embeddings, labels in test_loader:\n            inputs,embeddings, labels = inputs.to(device),embeddings.to(device), labels.to(device)\n            outputs = rafgic_model(inputs,embeddings)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item() * inputs.size(0)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    val_loss /= len(val_dataset)\n    val_accuracy = 100 * correct / total\n    \n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n    \n    # Check for improvement in validation loss\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        early_stopping_counter = 0\n        # Save the best model\n        torch.save(rafgic_model.state_dict(), 'rafgic_finetuned_best.pth')\n    else:\n        early_stopping_counter += 1\n        if early_stopping_counter >= patience:\n            print(f'Validation loss did not improve for {patience} epochs. Early stopping...')\n            break\n\n# Load the best model\nrafgic_model.load_state_dict(torch.load('rafgic_finetuned_best.pth'))","metadata":{"execution":{"iopub.status.busy":"2024-05-02T05:14:11.433884Z","iopub.execute_input":"2024-05-02T05:14:11.434679Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch [1/10], Step [10/255], Loss: 5.2941\nEpoch [1/10], Step [20/255], Loss: 5.1338\nEpoch [1/10], Step [30/255], Loss: 5.0220\nEpoch [1/10], Step [40/255], Loss: 4.8500\nEpoch [1/10], Step [50/255], Loss: 4.7737\nEpoch [1/10], Step [60/255], Loss: 4.5636\nEpoch [1/10], Step [70/255], Loss: 4.0922\nEpoch [1/10], Step [80/255], Loss: 3.7287\nEpoch [1/10], Step [90/255], Loss: 3.2014\nEpoch [1/10], Step [100/255], Loss: 3.5809\nEpoch [1/10], Step [110/255], Loss: 3.0550\nEpoch [1/10], Step [120/255], Loss: 2.5898\nEpoch [1/10], Step [130/255], Loss: 2.3502\nEpoch [1/10], Step [140/255], Loss: 2.0957\nEpoch [1/10], Step [150/255], Loss: 2.0625\nEpoch [1/10], Step [160/255], Loss: 1.7281\nEpoch [1/10], Step [170/255], Loss: 1.5734\nEpoch [1/10], Step [180/255], Loss: 1.4023\nEpoch [1/10], Step [190/255], Loss: 1.2878\nEpoch [1/10], Step [200/255], Loss: 1.0260\nEpoch [1/10], Step [210/255], Loss: 1.0121\nEpoch [1/10], Step [220/255], Loss: 1.2540\nEpoch [1/10], Step [230/255], Loss: 1.2241\nEpoch [1/10], Step [240/255], Loss: 0.9645\nEpoch [1/10], Step [250/255], Loss: 1.1706\nEpoch [1/10], Loss: 2.8122, Validation Loss: 1.2096, Validation Accuracy: 66.89%\nEpoch [2/10], Step [10/255], Loss: 0.7859\nEpoch [2/10], Step [20/255], Loss: 0.4982\nEpoch [2/10], Step [30/255], Loss: 0.7598\nEpoch [2/10], Step [40/255], Loss: 0.7273\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-05-01T17:30:11.959266Z","iopub.execute_input":"2024-05-01T17:30:11.959622Z","iopub.status.idle":"2024-05-01T17:38:45.137269Z","shell.execute_reply.started":"2024-05-01T17:30:11.959597Z","shell.execute_reply":"2024-05-01T17:38:45.136292Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Epoch [1/10], Loss: 1.4896, Validation Loss: 1.3841, Validation Accuracy: 74.62%\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}